# SFT Training Configuration for HARP
# Supervised Fine-Tuning on Qwen2.5-VL

model:
  name: "Qwen/Qwen2.5-VL-7B-Instruct"  # 或 3B/72B 变体
  freeze_vision_encoder: true
  use_lora: true

lora:
  rank: 64
  alpha: 128
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
  dropout: 0.05

data:
  train_path: "data/sft_train.jsonl"
  val_path: "data/sft_val.jsonl"

training:
  output_dir: "outputs/sft"
  num_epochs: 3
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_steps: 100
  max_length: 512
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  bf16: true
  report_to: ["tensorboard"]
  run_name: "sft_qwen2.5vl"
